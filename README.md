# Hi, I'm Varshitha Gudimalla ðŸ‘‹

## Data Engineer | Python | SQL | PySpark | AWS | Databricks

**varshithag1908@gmail.com** | [LinkedIn](https://linkedin.com/in/varshitha-gudimalla) | [Portfolio](https://varshitha-g.github.io/portfolio)

---

##  Education

- **MS in Data Science** - University at Albany, SUNY (May 2025)
- **BTech in Computer Science** - CMR Institute of Technology, Hyderabad (May 2022)

---

##  Current Role

### Data Engineer @ FedEx (Jan 2025 - Present)

Building high-complexity ETL pipelines and monitoring systems for mission-critical logistics operations:

- Built and modified high-complexity ETL pipelines using Python, PySpark, and SQL to ingest and transform high-volume transactional and PII datasets, ensuring strict privacy and security compliance for regulated data
- Developed automated monitoring and alerting systems using Apache Airflow and AWS Step Functions to detect data quality issues, unauthorized modifications, and pipeline failures, triggering task creation and email notifications to stakeholders
- Orchestrated end-to-end data workflows with Apache Airflow and Databricks Workflows, ensuring reliable daily ingestion, transformation, and validation with minimal manual intervention
- Collaborated cross-functionally with data scientists, analysts, and DevOps teams to understand data requirements, navigate ambiguity, and deliver scalable solutions supporting fraud detection and analytics use cases
- Implemented data quality frameworks using Great Expectations and Delta Lake constraints to ensure integrity across sensitive PII pipelines, with comprehensive documentation and version control via Git
- Engineered IAM roles and Unity Catalog access controls for secure data governance, maintaining compliance with privacy standards across multi-tenant production environments
- Worked autonomously to identify and prevent data loss incidents; optimized Spark performance and tuned cluster configurations to meet strict SLA requirements


---

##  Tech Stack

```
Languages:
  - Python, SQL, PySpark

Big Data & Processing:
  - Apache Spark, Delta Lake, Hive, SparkSQL

Cloud & Storage:
  - AWS: S3, Glue, Redshift, Lambda, Step Functions
  - Snowflake, Databricks

Workflow Orchestration:
  - Apache Airflow, Databricks Workflows, Jenkins

Data Quality & Governance:
  - Great Expectations, Glue Catalog, Hive Metastore

DevOps & Infrastructure:
  - Git, Terraform, CI/CD

Data Warehousing:
  - ETL pipeline development, warehouse infrastructure, metadata management

Security & Compliance:
  - IAM, data privacy, PII handling

Visualization:
  - Power BI, Tableau
```

---

##  Featured Projects

###  Global Inflation Monitor
**Spark | Snowflake | Tableau | ETL**

Built end-to-end Spark and Snowflake ETL pipelines with automated monitoring systems, delivering a Tableau dashboard enabling analysts to track real-time inflation and wage data across 190+ countries.

**Impact**: Reduced reporting delays by 80%

**Tech Stack**: `PySpark` `Snowflake` `Tableau` `Apache Airflow` `ETL`

---

###  Customer Churn Prediction
**Python | Machine Learning | Tableau | Predictive Modeling**

Developed predictive churn models and Tableau dashboards to surface high-risk telecom customers and behavioral patterns, empowering business analysts to drive targeted retention strategies and policy actions.

**Impact**: Enabled proactive retention strategies through data-driven insights

**Tech Stack**: `Python` `scikit-learn` `Tableau` `Predictive Modeling` `Feature Engineering`

---

##  Professional Experience

### Data Engineer | Knowledge Solutions (Jun 2022 - Jul 2023)
- Developed Python and PySpark-based ETL pipelines in Databricks to process ~50K weekly customer datasets, building scalable warehouse infrastructure for analytics and ML feature engineering
- Built and maintained end-to-end orchestration workflows using Apache Airflow to automate data movement from on-premise sources and APIs into AWS S3, ensuring reliable scheduled execution
- Implemented CI/CD pipelines with Jenkins for automated deployment of Databricks notebooks and job clusters across dev, test, and production environments
- Collaborated with data scientists and business analysts to prepare ML-ready feature tables, ensuring clear communication of technical requirements and data availability
- Integrated AWS Glue Catalog and Delta Lake for metadata management and version-controlled data lake architecture, supporting downstream analytics workloads
- Proactively identified data quality issues and implemented validation checks to prevent data loss and ensure pipeline reliability
- Built Power BI dashboards to track KPIs with scheduled updates, demonstrating ability to communicate technical insights to non-technical stakeholders


### Data Engineer | CloudEnd Platform Pvt Ltd (Jun 2021 - May 2022)
- Constructed ETL pipelines using Databricks, AWS Glue, PySpark, and SQL to process sensitive therapy logs and survey data into Delta Lake for compliance-driven analytics
- Ingested structured and semi-structured data from S3, flat files, and REST APIs, standardizing metadata in Glue Catalog and Hive Metastore
- Scheduled and monitored workflows using Apache Airflow DAGs with deployment automation via Jenkins for nightly refreshes and proactive alerting on failures
- Wrote optimized SQL procedures and views for patient segmentation metrics, supporting reporting teams with reliable warehouse infrastructure
- Maintained comprehensive Git-based version control, documentation, and IAM access policies aligned with healthcare data governance and privacy standards
- Built forecasting pipelines using Python and Prophet to predict patient outcomes, collaborating with clinical teams to translate ambiguous requirements into technical solutions


---

##  Career Highlights

| Year | Milestone | Focus |
|------|-----------|-------|
| 2025 | **Data Engineer @ FedEx** | High-complexity pipelines, monitoring systems, cross-functional collaboration |
| 2025 | **MS Data Science** | University at Albany (Completed) |
| 2022-23 | **Data Engineer @ Knowledge Solutions** | ETL infrastructure, warehouse development, ML feature engineering |
| 2021-22 | **Data Engineer @ CloudEnd Platform** | Healthcare compliance, data governance, forecasting pipelines |
| 2022 | **BTech Computer Science** | CMR Institute of Technology, India |

---

##  What I'm Working On

-  Building high-complexity ETL pipelines and monitoring systems at FedEx
-  Implementing robust data quality and security frameworks for sensitive data
-  Exploring advanced workflow orchestration with Apache Airflow
-  Enhancing warehouse infrastructure and metadata management practices
-  Contributing to data governance and privacy best practices

---

##  Let's Connect

I'm a Data Engineer with 3 years of experience building scalable data infrastructure, monitoring systems, and cross-functional collaboration on complex data projects.

- **Email**: varshithag1908@gmail.com
- **LinkedIn**: [varshitha-gudimalla](https://linkedin.com/in/varshitha-gudimalla)
- **Portfolio**: [varshitha-g.github.io/portfolio](https://varshitha-g.github.io/portfolio)
---

##  About Me

- Specialized in building high-complexity ETL pipelines with monitoring and alerting systems
- Experienced in handling sensitive data with strict privacy and security compliance (PII, HIPAA)
- Strong cross-functional collaborator comfortable navigating ambiguity with technical and non-technical stakeholders
- Proactive problem-solver focused on preventing data loss and ensuring pipeline reliability
- Passionate about data quality, governance, and building resilient warehouse infrastructure

---

 **If you find my work interesting, feel free to star my repositories!**

*Currently employed at FedEx | Open to new opportunities in Data Engineering*
