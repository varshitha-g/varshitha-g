# Hi, I'm Varshitha Gudimalla 👋

**Data Engineer** | AWS | Azure | Databricks | PySpark | SQL

varshithag1908@gmail.com | [LinkedIn](https://linkedin.com/in/varshitha-gudimalla-data-engineer) | [Portfolio](https://varshitha-g.github.io/portfolio/)

---

**🎓 Education:** MS in Data Science (University at Albany, 2025) | BTech in Computer Science (CMR Institute, 2022)

---

## 💼 Current Role

**Data Engineer @ FedEx** *(Jan 2025 - Present)*

Building real-time logistics data platform serving 40+ analytics and ML teams:

- AWS lakehouse processing **10-50TB daily** → Cut ETL time from 5 hours to 2 hours
- Real-time streaming with Kinesis → **$2M+ fraud prevention annually**
- 50+ Airflow DAGs → **99.9% pipeline reliability**
- Data quality framework → Passed SOC 2 audit, reduced costs 22%

---

## 🛠️ Tech Stack

```yaml
Cloud Platforms:
  - AWS: S3, Glue, Redshift, Kinesis, Lambda, EMR, Athena, CloudWatch
  - Azure: Databricks, Data Factory, Synapse Analytics, ADLS Gen2, DevOps
  - Snowflake

Big Data & Processing:
  - PySpark, Apache Spark, Spark SQL, Delta Lake, Hadoop, Hive

Programming:
  - Python (Pandas, NumPy, scikit-learn)
  - SQL (Advanced CTEs, Window Functions, Query Optimization)
  - Scala

Data Engineering:
  - ETL/ELT Pipelines, Data Lakes, Data Warehouses
  - CDC, SCD Type-2, Star/Snowflake Schema

Orchestration & Streaming:
  - Apache Airflow, Apache Kafka, AWS Kinesis, Jenkins

DevOps & Quality:
  - Git/GitHub, CI/CD, Docker, Terraform
  - Great Expectations, HIPAA/GDPR Compliance

Visualization:
  - Power BI, Tableau, Dashboard Development
```

---

## 📈 Featured Projects

### 🌍 Real-Time Economic Intelligence Platform
**Problem:** Analysts spending 5 days manually compiling inflation data from 190+ countries  
**Solution:** Built Spark + Snowflake pipeline processing 2M+ daily economic indicators  
**Impact:** 80% time reduction (5 days → 4 hours), enabling faster policy decisions  
**Tech:** `PySpark` `Snowflake` `REST APIs` `Airflow` `Python`

---

### 📞 ML-Powered Customer Retention System
**Problem:** Telecom losing 25% of customers annually with no early warning system  
**Solution:** End-to-end ML pipeline analyzing 500K+ customer behavioral patterns  
**Impact:** 87% prediction accuracy, saved **$2.3M annually**  
**Tech:** `PySpark` `Databricks` `scikit-learn` `Airflow` `Delta Lake`

---

## 🎯 Career Highlights

| Year | Milestone | Impact |
|------|-----------|--------|
| **2025** | Data Engineer @ FedEx | Processing 1B+ daily records, $2M+ fraud prevention |
| **2025** | MS Data Science | University at Albany (completed while working full-time) |
| **2022-23** | Data Engineer @ Knowledge Solutions | $4.8M revenue retained through ML churn prediction |
| **2021-22** | Data Engineer @ CloudEnd Platform | Zero HIPAA violations, 10-12% treatment improvement |
| **2022** | BTech Computer Science | CMR Institute of Technology, India |

---

## 📊 GitHub Stats

![Varshitha's GitHub stats](https://github-readme-stats.vercel.app/api?username=varshitha-g&show_icons=true&theme=blue-green&hide_border=true)

![Top Languages](https://github-readme-stats.vercel.app/api/top-langs/?username=varshitha-g&layout=compact&theme=blue-green&hide_border=true)
---

## 🌟 What I'm Working On

- 🔭 Building real-time data platforms at FedEx
- 🌱 Exploring advanced streaming architectures with Kafka and Flink
- 📚 Contributing to open-source data engineering tools
- 💡 Writing about data engineering best practices

---

## 📫 Let's Connect

I'm actively seeking new opportunities in Data Engineering focused on cloud platforms, real-time processing, and scalable data architecture.

- 📧 Email: [varshithag1908@gmail.com](mailto:varshithag1908@gmail.com)
- 💼 LinkedIn: [varshitha-gudimalla-data-engineer](https://linkedin.com/in/varshitha-gudimalla-data-engineer)
- 🌐 Portfolio: [varshitha-g.github.io/portfolio](https://varshitha-g.github.io/portfolio/)


---

## 💡 Fun Facts

- ⚙️ Built my first ETL pipeline processing patient data while ensuring HIPAA compliance
- 🎓 Completed MS in Data Science while working full-time as a Data Engineer
- 📊 Processed over 1 billion records in a single day at FedEx
- 🚀 Reduced data processing time from 5 hours to 2 hours through optimization

---

<div align="center">

### ⭐ If you find my work interesting, feel free to star my repositories!

**Currently employed at FedEx | Actively exploring new opportunities**

</div>
